<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Beyond the limits | BIG DATA ANALYSIS IN R TRAINING (BUSARA)</title>
  <meta name="description" content="This the text material that will accompany the trainign." />
  <meta name="generator" content="bookdown 0.16.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Beyond the limits | BIG DATA ANALYSIS IN R TRAINING (BUSARA)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This the text material that will accompany the trainign." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Beyond the limits | BIG DATA ANALYSIS IN R TRAINING (BUSARA)" />
  
  <meta name="twitter:description" content="This the text material that will accompany the trainign." />
  

<meta name="author" content="EDWIN KAGEREKI" />


<meta name="date" content="2019-12-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="taking-r-to-the-limit.html"/>
<link rel="next" href="stretching-the-limits.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Big Data Training in R - Busara</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> INTRODUCTION</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Understanding R’s Performance</a></li>
<li class="chapter" data-level="3" data-path="taking-r-to-the-limit.html"><a href="taking-r-to-the-limit.html"><i class="fa fa-check"></i><b>3</b> Taking R to the limit</a><ul>
<li class="chapter" data-level="3.1" data-path="taking-r-to-the-limit.html"><a href="taking-r-to-the-limit.html#managing-io"><i class="fa fa-check"></i><b>3.1</b> Managing I/O</a><ul>
<li class="chapter" data-level="3.1.1" data-path="taking-r-to-the-limit.html"><a href="taking-r-to-the-limit.html#benchmark-of-methods-for-file-import-and-export"><i class="fa fa-check"></i><b>3.1.1</b> benchmark of methods for file import and export</a></li>
<li class="chapter" data-level="3.1.2" data-path="taking-r-to-the-limit.html"><a href="taking-r-to-the-limit.html#versatile-data-import-with-rio"><i class="fa fa-check"></i><b>3.1.2</b> Versatile data import with rio</a></li>
<li class="chapter" data-level="3.1.3" data-path="taking-r-to-the-limit.html"><a href="taking-r-to-the-limit.html#divide-and-rule-preprocessing-text-outside-r"><i class="fa fa-check"></i><b>3.1.3</b> Divide and rule: Preprocessing text outside R</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="taking-r-to-the-limit.html"><a href="taking-r-to-the-limit.html#efficiently-set-up-pcs."><i class="fa fa-check"></i><b>3.2</b> Efficiently set-up PCs.</a></li>
<li class="chapter" data-level="3.3" data-path="taking-r-to-the-limit.html"><a href="taking-r-to-the-limit.html#efficiently-write-r-codes."><i class="fa fa-check"></i><b>3.3</b> Efficiently write R codes.</a><ul>
<li class="chapter" data-level="3.3.1" data-path="taking-r-to-the-limit.html"><a href="taking-r-to-the-limit.html#tips"><i class="fa fa-check"></i><b>3.3.1</b> TIPS</a></li>
<li class="chapter" data-level="3.3.2" data-path="taking-r-to-the-limit.html"><a href="taking-r-to-the-limit.html#avoid-duplication"><i class="fa fa-check"></i><b>3.3.2</b> Avoid duplication</a></li>
<li class="chapter" data-level="3.3.3" data-path="taking-r-to-the-limit.html"><a href="taking-r-to-the-limit.html#code-organisation"><i class="fa fa-check"></i><b>3.3.3</b> Code organisation</a></li>
<li class="chapter" data-level="3.3.4" data-path="taking-r-to-the-limit.html"><a href="taking-r-to-the-limit.html#memory-allocation"><i class="fa fa-check"></i><b>3.3.4</b> Memory allocation</a></li>
<li class="chapter" data-level="3.3.5" data-path="taking-r-to-the-limit.html"><a href="taking-r-to-the-limit.html#the-byte-compiler"><i class="fa fa-check"></i><b>3.3.5</b> The byte compiler</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="taking-r-to-the-limit.html"><a href="taking-r-to-the-limit.html#design-efficient-workflows."><i class="fa fa-check"></i><b>3.4</b> Design efficient workflows.</a></li>
<li class="chapter" data-level="3.5" data-path="taking-r-to-the-limit.html"><a href="taking-r-to-the-limit.html#code-optimisation"><i class="fa fa-check"></i><b>3.5</b> code optimisation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="beyond-the-limits.html"><a href="beyond-the-limits.html"><i class="fa fa-check"></i><b>4</b> Beyond the limits</a><ul>
<li class="chapter" data-level="4.1" data-path="beyond-the-limits.html"><a href="beyond-the-limits.html#introduction-to-hadoop-architecture."><i class="fa fa-check"></i><b>4.1</b> Introduction to Hadoop architecture.</a></li>
<li class="chapter" data-level="4.2" data-path="beyond-the-limits.html"><a href="beyond-the-limits.html#introduction-to-spark."><i class="fa fa-check"></i><b>4.2</b> Introduction to Spark.</a></li>
<li class="chapter" data-level="4.3" data-path="beyond-the-limits.html"><a href="beyond-the-limits.html#spark-vs-hadoop"><i class="fa fa-check"></i><b>4.3</b> Spark vs Hadoop</a></li>
<li class="chapter" data-level="4.4" data-path="beyond-the-limits.html"><a href="beyond-the-limits.html#running-r-in-spark."><i class="fa fa-check"></i><b>4.4</b> Running R in Spark.</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="stretching-the-limits.html"><a href="stretching-the-limits.html"><i class="fa fa-check"></i><b>5</b> Stretching the limits</a><ul>
<li class="chapter" data-level="5.1" data-path="stretching-the-limits.html"><a href="stretching-the-limits.html#stretching-the-processor"><i class="fa fa-check"></i><b>5.1</b> Stretching the Processor</a></li>
<li class="chapter" data-level="5.2" data-path="stretching-the-limits.html"><a href="stretching-the-limits.html#stretching-the-ram"><i class="fa fa-check"></i><b>5.2</b> Stretching the RAM</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="working-with-databases-in-r.html"><a href="working-with-databases-in-r.html"><i class="fa fa-check"></i><b>6</b> Working with databases in R</a><ul>
<li class="chapter" data-level="6.1" data-path="working-with-databases-in-r.html"><a href="working-with-databases-in-r.html#overview-of-the-types-of-databases."><i class="fa fa-check"></i><b>6.1</b> overview of the types of databases.</a><ul>
<li class="chapter" data-level="6.1.1" data-path="working-with-databases-in-r.html"><a href="working-with-databases-in-r.html#connecting-to-open-source-databases"><i class="fa fa-check"></i><b>6.1.1</b> Connecting to open source databases</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="working-with-databases-in-r.html"><a href="working-with-databases-in-r.html#connecting-to-databases"><i class="fa fa-check"></i><b>6.2</b> Connecting to databases</a></li>
<li class="chapter" data-level="6.3" data-path="working-with-databases-in-r.html"><a href="working-with-databases-in-r.html#database-queries-with-r"><i class="fa fa-check"></i><b>6.3</b> Database Queries With R</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="working-with-databases-in-r-1.html"><a href="working-with-databases-in-r-1.html"><i class="fa fa-check"></i><b>7</b> Working with databases in R</a><ul>
<li class="chapter" data-level="7.1" data-path="working-with-databases-in-r-1.html"><a href="working-with-databases-in-r-1.html#overview-of-the-types-of-databases.-1"><i class="fa fa-check"></i><b>7.1</b> Overview of the types of databases.</a><ul>
<li class="chapter" data-level="7.1.1" data-path="working-with-databases-in-r-1.html"><a href="working-with-databases-in-r-1.html#database-interface"><i class="fa fa-check"></i><b>7.1.1</b> Database Interface</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="working-with-databases-in-r-1.html"><a href="working-with-databases-in-r-1.html#connecting-to-databases-1"><i class="fa fa-check"></i><b>7.2</b> Connecting to databases</a></li>
<li class="chapter" data-level="7.3" data-path="working-with-databases-in-r-1.html"><a href="working-with-databases-in-r-1.html#read-write-and-modify-database"><i class="fa fa-check"></i><b>7.3</b> Read, write and modify database</a></li>
<li class="chapter" data-level="7.4" data-path="working-with-databases-in-r-1.html"><a href="working-with-databases-in-r-1.html#database-queries-with-r-1"><i class="fa fa-check"></i><b>7.4</b> Database Queries With R</a></li>
<li class="chapter" data-level="7.5" data-path="working-with-databases-in-r-1.html"><a href="working-with-databases-in-r-1.html#modeling-data-with-modeldb-tidypredict"><i class="fa fa-check"></i><b>7.5</b> modeling data with modeldb &amp; tidypredict</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="big-data-analysis-data-analysis-visualization-and-deployment.html"><a href="big-data-analysis-data-analysis-visualization-and-deployment.html"><i class="fa fa-check"></i><b>8</b> Big data analysis: Data analysis, visualization and deployment</a><ul>
<li class="chapter" data-level="8.1" data-path="big-data-analysis-data-analysis-visualization-and-deployment.html"><a href="big-data-analysis-data-analysis-visualization-and-deployment.html#data-sources."><i class="fa fa-check"></i><b>8.1</b> Data sources.</a></li>
<li class="chapter" data-level="8.2" data-path="big-data-analysis-data-analysis-visualization-and-deployment.html"><a href="big-data-analysis-data-analysis-visualization-and-deployment.html#create-retrieve-and-manage-big-data."><i class="fa fa-check"></i><b>8.2</b> Create, retrieve and manage big data.</a></li>
<li class="chapter" data-level="8.3" data-path="big-data-analysis-data-analysis-visualization-and-deployment.html"><a href="big-data-analysis-data-analysis-visualization-and-deployment.html#run-typical-big-data-analysis-descriptive-and-models-using-r."><i class="fa fa-check"></i><b>8.3</b> Run typical big data analysis: descriptive and models using R.</a></li>
<li class="chapter" data-level="8.4" data-path="big-data-analysis-data-analysis-visualization-and-deployment.html"><a href="big-data-analysis-data-analysis-visualization-and-deployment.html#unique-aspects-of-big-data-visualization."><i class="fa fa-check"></i><b>8.4</b> Unique aspects of big data visualization.</a></li>
<li class="chapter" data-level="8.5" data-path="big-data-analysis-data-analysis-visualization-and-deployment.html"><a href="big-data-analysis-data-analysis-visualization-and-deployment.html#introduction-to-reactive-tables-and-graphs."><i class="fa fa-check"></i><b>8.5</b> Introduction to reactive tables and graphs.</a></li>
<li class="chapter" data-level="8.6" data-path="big-data-analysis-data-analysis-visualization-and-deployment.html"><a href="big-data-analysis-data-analysis-visualization-and-deployment.html#deployment-platforms-web-applications-reports-mobile-applications."><i class="fa fa-check"></i><b>8.6</b> Deployment platforms: Web applications, reports, mobile applications.</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="planning-and-implementing-big-data-analysis.html"><a href="planning-and-implementing-big-data-analysis.html"><i class="fa fa-check"></i><b>9</b> Planning and implementing Big Data analysis</a><ul>
<li class="chapter" data-level="9.1" data-path="planning-and-implementing-big-data-analysis.html"><a href="planning-and-implementing-big-data-analysis.html#need-assessment."><i class="fa fa-check"></i><b>9.1</b> Need assessment.</a></li>
<li class="chapter" data-level="9.2" data-path="planning-and-implementing-big-data-analysis.html"><a href="planning-and-implementing-big-data-analysis.html#designing-a-workflow."><i class="fa fa-check"></i><b>9.2</b> Designing a workflow.</a></li>
<li class="chapter" data-level="9.3" data-path="planning-and-implementing-big-data-analysis.html"><a href="planning-and-implementing-big-data-analysis.html#case-scenarios."><i class="fa fa-check"></i><b>9.3</b> Case scenarios.</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">BIG DATA ANALYSIS IN R TRAINING (BUSARA)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="beyond-the-limits" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Beyond the limits</h1>
<div class="alert alert alert-info">
<p>
Chapter objectives:
</p>
</div>
<p>In this chapter we will go to the very limits of large-scale data processing. The term Big Data has been used to describe the ever growing volume, velocity, and variety of data being generated on the Internet in connected devices and many other places. Many organizations now have massive datasets that measure in petabytes (one petabyte is 1,048,576 gigabytes), more than ever before. Processing and analyzing Big Data is extremely challenging for traditional data processing tools and database architectures. We have already established that large datasets of size petabytes cannot be loaded into the RAM memory; this is when Hadoop integrated with R language, is an ideal solution. To adapt to the in-memory, single machine limitation of R programming language, data scientists have to limit their data analysis to a sample of data from the large data set. This limitation of R programming language comes as a major hindrance when dealing with big data. Since, R is not very scalable, the core R engine can process only limited amount of data.</p>
<p>To the contrary, distributed processing frameworks like Hadoop are scalable for complex operations and tasks on large datasets (petabyte range) but do not have strong statistical analytical capabilities. As Hadoop is a popular framework for big data processing, integrating R with Hadoop is the next logical step. Using R on Hadoop will provide highly scalable data analytics platform which can be scaled depending on the size of the dataset. Integrating Hadoop with R lets data scientists run R in parallel on large dataset as none of the data science libraries in R language will work on a dataset that is larger than its memory. Big Data analytics with R and Hadoop competes with the cost value return offered by commodity hardware cluster for vertical scaling.</p>
<p>Hadoop and its ecosystem of tools is rapidly evolving. Other tools are being actively developed to make Hadoop perform even better. For example, Apache Spark (<a href="http://spark.apache.org/" class="uri">http://spark.apache.org/</a>) provides Resilient Distributed Datasets (RDDs) that store data in memory across a Hadoop cluster. This allows data to be read from HDFS once and to be used many times in order to dramatically improve the performance of interactive tasks like data exploration and iterative algorithms like gradient descent or k-means clustering.</p>
<div id="introduction-to-hadoop-architecture." class="section level2">
<h2><span class="header-section-number">4.1</span> Introduction to Hadoop architecture.</h2>
<p>The Hadoop ecosystem elements described below are all open system Apache Hadoop Project. Many commercial applications use these ecosystem elements. Let us summarize Hadoop ecosystem components. At the core, we have HDFS for data storage, map-reduce for data processing and Yarn a resource manager. Then we have HIVE a data analysis tool, Pig – SQL like a scripting language, HBase – NoSQL database, Mahout – machine learning tool, Zookeeper – a synchronization tool, Oozie – workflow scheduler system, Sqoop – structured data importing and exporting utility, Flume – data transfer tool for unstructured and semi-structured data, Ambari – a tool for managing and securing Hadoop clusters, and lastly Avro – RPC, and data serialization framework.</p>
<p><strong>RHadoop</strong></p>
<p>RHadoop is a collection of five R packages that allow users to manage and analyze data with Hadoop. The packages have been tested (and always before a release) on recent releases of the Cloudera and Hortonworks Hadoop distributions and should have broad compatibility with open source Hadoop and mapR’s distribution. We normally test on recent Revolution R/Microsoft R and CentOS releases, but we expect all the RHadoop packages to work on a recent release of open source R and Linux.</p>
<p>RHadoop consists of the following packages:</p>
<p>rhdfs This package provides basic connectivity to the Hadoop Distributed File System. R programmers can browse, read, write, and modify files stored in HDFS from within R. Install this package only on the node that will run the R client. rhbase This package provides basic connectivity to the HBASE distributed database, using the Thrift server. R programmers can browse, read, write, and modify tables stored in HBASE from within R. Install this package only on the node that will run the R client. plyrmr This package enables the R user to perform common data manipulation operations, as found in popular packages such as plyr and reshape2, on very large data sets stored on Hadoop. Like rmr, it relies on Hadoop MapReduce to perform its tasks, but it provides a familiar plyr-like interface while hiding many of the MapReduce details. Install this package only every node in the cluster. rmr2 A package that allows R developer to perform statistical analysis in R via Hadoop MapReduce functionality on a Hadoop cluster. Install this package on every node in the cluster. ravro A package that adds the ability to read and write avro files from local and HDFS file system and adds an avro input format for rmr2. Install this package only on the node that will run the R client.</p>
<p><strong>Using Hadoop with R: It Depends.</strong></p>
<p>In this post, I’ll summarize the alternatives using pure open source R and some of their advantages. In a subsequent post, I’ll describe the options for achieving even greater scale, speed, stability and ease of development by combining open source and commercial technologies.</p>
<p>These two posts are written to help current R users who are novices at Hadoop understand and select solutions to evaluate.</p>
<p>As with most thing open source, the first consideration is of course monetary. Isn’t it always? The good news is that there are multiple alternatives that are free, and additional capabilities under development in various open source projects.</p>
<p>We see generally 4 options for building R to Hadoop integration using entirely open source stacks.</p>
<p>Option 1: Install R on workstations and connect to data in Hadoop.</p>
<p>This baseline approach’s greatest advantage is simplicity and cost. It’s free. End to end free. What else in life is?</p>
<p>Through packages Revolution contributed to open source including rhdfs and rhbase, R users can directly ingest data from both the hdfs file system and the hbase database subsystems in Hadoop. Both connectors are part of the RHadoop package created and maintained by Revolution and are a go-to choice.</p>
<p>Additional options exist as well. The RHive package executes Hive’s HQL SQL-like query language directly from R, and provides functions for retrieving metadata from Hive such as database names, table names, column names, etc.</p>
<p>The rhive package, in particular, has the advantage that its data operations some work to be pushed down into Hadoop, avoiding data movement and parallelizing operations for big speed increases. Similar “push-down” can be achieved with rhbase as well. However, neither are particularly rich environments, and invariably, complex analytical problems will reveal some gaps in capability.</p>
<p>Beyond the somewhat limited push-down capabilities, R’s best at working on modest data sampled from hdfs, hbase or hive, and in this way, current R users can get going with Hadoop quickly.</p>
<p>Option 2: Install R on a shared server and connect to Hadoop. Once you tire of R’s memory barriers on your laptop the obvious next path is a shared server. With today’s technologies, you can equip a powerful server for only a few thousand dollars, and easily share it between a few users. Using Windows or Linux with 256GB, 512GB of RAM, R can be used to analyze files in to the hundreds of gigabytes, albeit not as fast as perhaps you’d like.</p>
<p>Like option 1, R on a shared server can also leverage push-down capabilities of the rhbase and rhive packages to achieve parallelism and avoid data movement. However, as with workstations, the pushdown capabilities of rhive and rhbase are limited.</p>
<p>And of course, while lots of RAM keeps the dread out of memory exhustion at bay, it does little for compute performance, and depends on sharing skills learned [or perhaps not learned] in kindergarten. For these reasons, consider a shared server to be a great add-on to R on workstations but not a complete substitute.</p>
<p>Option 3: Utilize Revolution R Open Replacing the CRAN download of R with the R distribution: Revolution R Open (RRO) enhances performance further. RRO is, like R itself, open source and 100% R and free for the download. It accelerates math computations using the Intel Math Kernel Libraries and is 100% compatible with the algorithms in CRAN and other repositories like BioConductor. No changes are required to R scripts, and the acceleration the MKL libraries offer varies from negligible to an order of magnitude for scripts making intensive use of certain math and linear algebra primitives. You can anticipate that RRO can double your average performance if you’re doing math operations in the language.</p>
<p>As with options 1 and 2, Revolution R Open can be used with connectors like rhdfs, and can connect and push work down into Hadoop through rhbase and rhive.</p>
<p>Option 4: Execute R inside of MapReduce using RMR2. Once you find that your problem set is too big, or your patience is being taxed on a workstation or server and the limitations of rhbase and rhive push down are impeding progress, you’re ready for running R inside of Hadoop.</p>
<p>The open source RHadoop project that includes rhdfs, rhbase and plyrmr also includes a package rmr2 that enables R users to build Hadoop map and reduce operations using R functions. Using mappers, R functions are applied to all of the data blocks that compose an hdfs file, an hbase table or other data sets, and the results can be sent to a reducer, also an R function, for aggregation or analysis. All work is conducted inside of Hadoop but is built in R.</p>
<p>Let’s be clear. Applying R functions on each hdfs file segment is a great way to accelerate computation. But for most, it is the avoidance of moving data that really accentuates performance. To do this, rmr2 applies R functions to the data residing on Hadoop nodes rather than moving the data to where R resides.</p>
<p>While rmr2 gives essentially unlimited capabilities, as a data scientist or statistician, your thoughts will soon turn to computing entire algorithms in R on large data sets. To use rmr2 in this way complicates development, for the R programmer because he or she must write the entire logic of the desired algorithm or adapt existing CRAN algorithms. She or he must then validate that the algorithm is accurate and reflects the expected mathematical result, and write code for the myriad corner cases such as missing data.</p>
<p>rmr2 requires coding on your part to manage parallelization. This may be trivial for data transformation operations, aggregates, etc., or quite tedious if you’re trying to train predictive models or build classifiers on large data.</p>
<p>While rmr2 can be more tedious than other approaches, it is not untenable, and most R programmers will find rmr2 much easier than resorting to Java-based development of Hadoop mappers and reducers. While somewhat tedious, it is a) fully open source, b) helps to parallelize computation to address larger data sets, c) skips painful data movement, d) is broadly used so you’ll find help available, and e), is free. Not bad.</p>
<p>rmr2 is not the only option in this category – a similar package called rhipe is also and provides similar capabilities. rhipe is described here and here and is downloadable from GitHub.</p>
<p><a href="https://blog.revolutionanalytics.com/2015/06/using-hadoop-with-r-it-depends.html" class="uri">https://blog.revolutionanalytics.com/2015/06/using-hadoop-with-r-it-depends.html</a></p>
</div>
<div id="introduction-to-spark." class="section level2">
<h2><span class="header-section-number">4.2</span> Introduction to Spark.</h2>
<p>Spark Programming is a general-purpose &amp; lightning fast cluster computing platform. In other words, it is an open source, wide range data processing engine. That reveals development API’s, which also qualifies data workers to accomplish streaming, machine learning or SQL workloads which demand repeated access to data sets. However, Spark can perform batch processing and stream processing. Batch processing refers, to the processing of the previously collected job in a single batch. Whereas stream processing means to deal with Spark streaming data.</p>
<p>Moreover, it is designed in such a way that it integrates with all the Big data tools. Like spark can access any Hadoop data source, also can run on Hadoop clusters. Furthermore, Apache Spark extends Hadoop MapReduce to the next level. That also includes iterative queries and stream processing. One more common belief about Spark is that it is an extension of Hadoop. Although that is not true. However, Spark is independent of Hadoop since it has its own cluster management system. Basically, it uses Hadoop for storage purpose only.</p>
<p>Although, there is one spark’s key feature that it has in-memory cluster computation capability. Also increases the processing speed of an application.</p>
<p>Basically, Apache Spark offers high-level APIs to users, such as Java, Scala, Python, and R. Although, Spark is written in Scala still offers rich APIs in Scala, Java, Python, as well as R. We can say, it is a tool for running spark applications.</p>
<p>Most importantly, by comparing Spark with Hadoop, it is 100 times faster than Hadoop In-Memory mode and 10 times faster than Hadoop On-Disk mode.</p>
<p><strong>Apache Spark Components</strong></p>
<p>In this Apache Spark Tutorial, we discuss Spark Components. It puts the promise for faster data processing as well as easier development. It is only possible because of its components. All these Spark components resolved the issues that occurred while using Hadoop MapReduce.</p>
<p>Now let’s discuss each Spark Ecosystem Component one by one-</p>
<ol style="list-style-type: lower-alpha">
<li><p>Spark Core Spark Core is a central point of Spark. Basically, it provides an execution platform for all the Spark applications. Moreover, to support a wide array of applications, Spark Provides a generalized platform.</p></li>
<li><p>Spark SQL On the top of Spark, Spark SQL enables users to run SQL/HQL queries. We can process structured as well as semi-structured data, by using Spark SQL. Moreover, it offers to run unmodified queries up to 100 times faster on existing deployments.</p></li>
<li><p>Spark Streaming Basically, across live streaming, Spark Streaming enables a powerful interactive and data analytics application. Moreover, the live streams are converted into micro-batches those are executed on top of spark core.</p></li>
<li><p>Spark MLlib Machine learning library delivers both efficiencies as well as the high-quality algorithms. Moreover, it is the hottest choice for a data scientist. Since it is capable of in-memory data processing, that improves the performance of iterative algorithm drastically.</p></li>
<li><p>SparkR Basically, to use Apache Spark from R. It is R package that gives light-weight frontend. Moreover, it allows data scientists to analyze large datasets. Also allows running jobs interactively on them from the R shell. Although, the main idea behind SparkR was to explore different techniques to integrate the usability of R with the scalability of Spark.</p></li>
<li><p>Spark GraphX Basically, Spark GraphX is the graph computation engine built on top of Apache Spark that enables to process graph data at scale.</p></li>
</ol>
</div>
<div id="spark-vs-hadoop" class="section level2">
<h2><span class="header-section-number">4.3</span> Spark vs Hadoop</h2>
</div>
<div id="running-r-in-spark." class="section level2">
<h2><span class="header-section-number">4.4</span> Running R in Spark.</h2>
<ol style="list-style-type: lower-alpha">
<li>Installing Java</li>
</ol>
<p>I will hereby demonstrate how to setup Spark and run R within it, both as a stand alone and as a cluster.</p>
<p>Spark is built in the Scala programming language, which is run by the Java Virtual Machine (JVM), you also need to install Java 8 on your system. It is likely that your system already has Java installed, but you should still check the version and update or downgrade as described in <a href="https://www.java.com/en/download/">Installing Java</a>. You can use the following R command to check which version is installed on your system:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">system</span>(<span class="st">&quot;java -version&quot;</span>)</code></pre></div>
<p>You can also use the JAVA_HOME environment variable to point to a specific Java version by running Sys.setenv(JAVA_HOME = “path-to-java-8”); either way, before moving on to installing sparklyr, make sure that Java 8 is the version available for R.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Installing sparklyr</li>
</ol>
<p>As with many other R packages, you can install sparklyr from CRAN as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">install.packages</span>(<span class="st">&quot;sparklyr&quot;</span>)</code></pre></div>
<pre><code>## Installing package into &#39;/home/rstudio/R/x86_64-pc-linux-gnu-library/3.5&#39;
## (as &#39;lib&#39; is unspecified)</code></pre>
<p>The examples in this book assume you are using the latest version of sparklyr. You can verify your version is as new as the one we are using by running the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">packageVersion</span>(<span class="st">&quot;sparklyr&quot;</span>)</code></pre></div>
<pre><code>## [1] &#39;1.0.5&#39;</code></pre>
<ol start="3" style="list-style-type: lower-alpha">
<li>Installing Spark</li>
</ol>
<p>Start by loading sparklyr:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)</code></pre></div>
<p>This makes all sparklyr functions available in R, which is really helpful; otherwise, you would need to run each sparklyr command prefixed with sparklyr::.</p>
<p>You can easily install Spark by running spark_install(). This downloads, installs, and configures the latest version of Spark locally on your computer; however, because we’ve written this book with Spark 2.3, you should also install this version to make sure that you can follow all the examples provided without any surprises:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_available_versions</span>() <span class="co"># To check the versions available for instalation</span></code></pre></div>
<pre><code>##   spark
## 1   1.6
## 2   2.0
## 3   2.1
## 4   2.2
## 5   2.3
## 6   2.4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_install</span>(<span class="st">&quot;2.4&quot;</span>) <span class="co"># You may install a specific version by altering that option</span>
<span class="kw">spark_installed_versions</span>() <span class="co"># To check the installed verion</span></code></pre></div>
<pre><code>##   spark hadoop                                           dir
## 1 2.4.0    2.7 /home/rstudio/spark/spark-2.4.0-bin-hadoop2.7
## 2 2.4.3    2.7 /home/rstudio/spark/spark-2.4.3-bin-hadoop2.7</code></pre>
<ol start="3" style="list-style-type: lower-alpha">
<li>Connecting</li>
</ol>
<p>It’s important to mention that, so far, we’ve installed only a local Spark cluster. A local cluster is really helpful to get started, test code, and troubleshoot with ease. Later chapters explain where to find, install, and connect to real Spark clusters with many machines, but for the first few chapters, we focus on using local clusters.</p>
<p>The following are the recommended Spark properties to set when connecting via R:</p>
<ul>
<li><p>sparklyr.cores.local - It defaults to using all of the available cores. Not a necessary property to set, unless there’s a reason to use less cores than available for a given Spark session.</p></li>
<li><p>sparklyr.shell.driver-memory - The limit is the amount of RAM available in the computer minus what would be needed for OS operations.</p></li>
<li><p>spark.memory.fraction - The default is set to 60% of the requested memory per executor. For more information, please see this Memory Management Overview page in the official Spark website</p></li>
</ul>
<p>To connect to this local cluster, simply run the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)

conf &lt;-<span class="st"> </span><span class="kw">spark_config</span>()
conf<span class="op">$</span><span class="st">`</span><span class="dt">sparklyr.cores.local</span><span class="st">`</span> &lt;-<span class="st"> </span><span class="dv">4</span>
conf<span class="op">$</span><span class="st">`</span><span class="dt">sparklyr.shell.driver-memory</span><span class="st">`</span> &lt;-<span class="st"> &quot;16G&quot;</span>
conf<span class="op">$</span>spark.memory.fraction &lt;-<span class="st"> </span><span class="fl">0.9</span>

sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, 
                    <span class="dt">version =</span> <span class="st">&quot;2.4&quot;</span>,
                    <span class="dt">config =</span> conf)


<span class="co">#sc &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.4&quot;)</span>
<span class="co">#spark_web(sc)</span></code></pre></div>
<p>After a connection is established, spark_connect() retrieves an active Spark connection, which most code usually names sc; you will then make use of the connection to execute Spark commands.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Using Spark</li>
</ol>
<p>Now that you are connected, we can run a few simple commands. For instance, let’s start by copying the mtcars dataset into Apache Spark by using copy_to():</p>
<p><strong>Reading data</strong> The data was copied into Spark, but we can access it from R using the data reference.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data&lt;-<span class="kw">read.csv</span>(<span class="st">&quot;data3.csv&quot;</span>)
data &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, data)
data</code></pre></div>
<pre><code>## # Source: spark&lt;data&gt; [?? x 5]
##    mta_tax tip_amount tolls_amount improvement_surcharge total_amount
##      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;                 &lt;dbl&gt;        &lt;dbl&gt;
##  1   NaN       NaN          NaN                    NaN          NaN  
##  2     0.5       0            0                      0.3         21.8
##  3     0.5       0            0                      0.3         36.3
##  4     0.5       5.05         0                      0.3         25.4
##  5     0.5       0            0                      0.3          8.3
##  6     0.5       2.45         0                      0.3         14.8
##  7     0.5       0            0                      0.3         55.3
##  8     0.5       0            0                      0.3         10.3
##  9     0.5      11.7          5.76                   0.3         70.3
## 10     0.5       3.36         0                      0.3         20.2
## # … with more rows</code></pre>
<p>You have successfully connected and loaded your first dataset into Spark.</p>
<p>Let’s explain what’s going on in copy_to(). The first parameter, sc, gives the function a reference to the active Spark connection that was created earlier with spark_connect(). The second parameter specifies a dataset to load into Spark. Now, copy_to() returns a reference to the dataset in Spark, which R automatically prints. Whenever a Spark dataset is printed, Spark collects some of the records and displays them for you.</p>
<p><strong>Simple analysis</strong></p>
<p>When using Spark from R to analyze data, you can use SQL (Structured Query Language) or dplyr (a grammar of data manipulation). You can use SQL through the DBI package; for instance, to count how many records are available in our cars dataset, we can run the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(DBI)
<span class="kw">dbGetQuery</span>(sc, <span class="st">&quot;SELECT count(*) FROM data&quot;</span>)</code></pre></div>
<pre><code>##   count(1)
## 1      714</code></pre>
<p><a href="https://therinspark.com/clusters.html" class="uri">https://therinspark.com/clusters.html</a></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="taking-r-to-the-limit.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="stretching-the-limits.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
